{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from osgeo import gdal, osr\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY_STAGING = ''\n",
    "API_KEY = ''\n",
    "\n",
    "HEADERS = {\n",
    "            \"username\": '__key__',\n",
    "            \"password\": API_KEY,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "DATA_HEADERS = {\n",
    "            \"username\": '__key__',\n",
    "            \"password\": API_KEY,\n",
    "}\n",
    "\n",
    "\n",
    "RASTER_SOURCES_ENDPOINT = 'https://demo.lizard.net/api/v4/rastersources/'\n",
    "RASTER_SOURCES_DATA_ENDPOINT = 'https://demo.lizard.net/api/v4/rastersources/{uuid}/data/'\n",
    "RASTER_LAYERS_ENDPOINT = 'https://demo.lizard.net/api/v4/rasters/'\n",
    "LAYERCOLLECTIONS_ENDPOINT = 'https://demo.lizard.net/api/v4/layercollections/'\n",
    "ORGANISATIONS_ENDPOINT = 'https://demo.lizard.net/api/v4/organisations/'\n",
    "\n",
    "NENS_ORGANISATION_STAGING = '61f5a464-c350-44c1-9bc7-d4b42d7f58cb'\n",
    "\n",
    "MAXWAITTIME = 3*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asc_to_tif(asc_fn, tif_fn):\n",
    "    drv = gdal.GetDriverByName('GTiff')\n",
    "    ds_in = gdal.Open(asc_fn)\n",
    "    ds_out = drv.CreateCopy(tif_fn, ds_in)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(28992)\n",
    "    ds_out.SetProjection(srs.ExportToWkt())\n",
    "    ds_out.GetRasterBand(1).SetNoDataValue(-999.9990)\n",
    "    ds_in = None\n",
    "    ds_out = None\n",
    "    \n",
    "def set_crs_for_tif(tif_fn):\n",
    "    drv = gdal.GetDriverByName('GTiff')\n",
    "    ds_out = gdal.Open(tif_fn, 1)\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromEPSG(28992)\n",
    "    ds_out.SetProjection(srs.ExportToWkt())\n",
    "    ds_out = None\n",
    " \n",
    "def post_raster_source(name, description, organisation_uuid, temporal, interval=None, headers=HEADERS):    \n",
    "\n",
    "    if temporal:\n",
    "        raster_source =  {'name': name,\n",
    "                          'description': description,\n",
    "                          'organisation' : organisation_uuid,\n",
    "                          'temporal': temporal,\n",
    "                          'interval': interval}\n",
    "    else:\n",
    "        raster_source =  {'name': name,\n",
    "                          'description': description,\n",
    "                          'organisation' : organisation_uuid,\n",
    "                          'temporal': temporal}\n",
    "\n",
    "        \n",
    "    r = requests.post(RASTER_SOURCES_ENDPOINT, json.dumps(raster_source), headers=headers)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def post_raster_layer(name, description, organisation_uuid, observation_type, source_uuid, style, layer_collection_slug, headers=HEADERS):\n",
    "    raster_layer =  {'name': name,\n",
    "                  'description': description,\n",
    "                 'organisation' : organisation_uuid,\n",
    "                 'observation_type': observation_type,\n",
    "                 'source': {\n",
    "                        \"name\": \"RasterStoreSource\",\n",
    "                        \"graph\": {\n",
    "                            \"RasterStoreSource\": [\n",
    "                                \"lizard_nxt.blocks.LizardRasterSource\",\n",
    "                                source_uuid\n",
    "                            ]\n",
    "                  }},\n",
    "                 'options': {\"styles\": style},\n",
    "                 'layer_collections': [layer_collection_slug]\n",
    "    }\n",
    "\n",
    "    r = requests.post(RASTER_LAYERS_ENDPOINT, json.dumps(raster_layer), headers=headers)\n",
    "                     \n",
    "    return r\n",
    "\n",
    "def get_rastersource_uuid_scenario(scenario_id, variable, headers=HEADERS):\n",
    "    params = {'name__endswith': 'ID:{}'.format(scenario_id),\n",
    "              'name__icontains': variable}\n",
    "            \n",
    "    r = requests.get('https://demo.lizard.net/api/v4/rastersources', params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    return r.json()['results'][0]['uuid']\n",
    "\n",
    "def get_rasterlayer_uuid_scenario(scenario_id, variable, headers=HEADERS):\n",
    "    params = {'name__endswith': 'ID:{}'.format(scenario_id),\n",
    "              'name__icontains': variable}\n",
    "            \n",
    "    r = requests.get('https://demo.lizard.net/api/v4/rasters', params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    return r.json()['results'][0]['uuid']\n",
    "\n",
    "def delete_raster_with_scenario_id_and_variable(scenario_id, variable, headers=HEADERS):\n",
    "    \n",
    "    params = {'name__endswith': 'ID:{}'.format(scenario_id),\n",
    "              'name__icontains': variable}\n",
    "            \n",
    "    r = requests.get('https://demo.lizard.net/api/v4/rasters', params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    if r.json()['count'] > 0:\n",
    "        \n",
    "        for result in r.json()['results']:\n",
    "            raster_layer_url = result['url']\n",
    "            r_layer_delete = requests.delete(raster_layer_url, headers=headers)\n",
    "            r_layer_delete.raise_for_status()\n",
    "            print('Removed rasterlayer {}...'.format(raster_layer_url))\n",
    "\n",
    "    r = requests.get('https://demo.lizard.net/api/v4/rastersources', params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    if r.json()['count'] > 0:\n",
    "        for result in r.json()['results']:\n",
    "            raster_source_url = result['url']\n",
    "            r_source_delete = requests.delete(raster_source_url, headers=headers)\n",
    "            r_source_delete.raise_for_status()\n",
    "            print('Removed rastersource {}...'.format(raster_source_url))\n",
    "\n",
    "\n",
    "def delete_organisation_rasters(organisation_uuid):\n",
    "    \n",
    "    params = {'organisation__uuid': organisation_uuid}\n",
    "    r = requests.get('https://demo.lizard.net/api/v4/rasters', params=params)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    rasters\n",
    "    \n",
    "    \n",
    "def create_or_return_layer_collection(slug, organisation_uuid, headers=HEADERS):\n",
    "\n",
    "    params = {'slug':slug}\n",
    "    r = requests.get(LAYERCOLLECTIONS_ENDPOINT, params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    if len(r.json()['results']) > 0:\n",
    "        collection = r.json()['results'][0]\n",
    "        return collection['slug']\n",
    "    else:    \n",
    "        collection =  {'slug': slug,\n",
    "                       'organisation' : organisation_uuid}\n",
    "\n",
    "        r = requests.post(LAYERCOLLECTIONS_ENDPOINT, json.dumps(collection), headers=headers)\n",
    "        return r.json()['slug']\n",
    "    \n",
    "def search_organisation_by_name(organisation_name, headers=HEADERS):\n",
    "    \n",
    "    full_name = 'LDO - ' + organisation_name\n",
    "    params = {'name':full_name}\n",
    "    r = requests.get(ORGANISATIONS_ENDPOINT, params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    \n",
    "    if r.json()['count'] > 0:\n",
    "        result = r.json()['results'][0]\n",
    "        uuid = result['uuid']\n",
    "        return uuid\n",
    "    else:\n",
    "        return None    \n",
    "    \n",
    "def post_data_to_source(source_uuid, file):                 \n",
    "\n",
    "    files = {\"file\": open(file, \"rb\")}\n",
    "    url = RASTER_SOURCES_DATA_ENDPOINT.format(uuid=source_uuid)\n",
    "    r = requests.post(url=url, files=files, headers={'username': '__key__','password': API_KEY})\n",
    "    \n",
    "    return r\n",
    "                    \n",
    "\n",
    "def upload_rasterperiod(url, df):\n",
    "    \"\"\"\n",
    "    Function for uploading a period of rasters.\n",
    "    Uploads the next one after the previous one succeeds, as the\n",
    "    Lizard API often crashes otherwise. \n",
    "    Inputs are the url to upload (with uuid and /data/) to and\n",
    "    a pandas dataframe. This df should have pandas datetime as index, and\n",
    "    a column named file which contains the file path relative to cwd.\n",
    "    All other columns are returned except columns with keywords task, status and waittime. \n",
    "    \"\"\"\n",
    "    \n",
    "    upload_df = df.sort_index()\n",
    "    for index, row in upload_df.iterrows():\n",
    "        file = {\"file\": open(row[\"file\"], \"rb\")}\n",
    "        data = {\"timestamp\": index.isoformat() + \"Z\"}\n",
    "        print(\"Posting for {}\".format(index))\n",
    "        \n",
    "        try:\n",
    "            r = requests.post(url=url, data=data, files=file, headers=DATA_HEADERS)\n",
    "            r.raise_for_status()\n",
    "            upload_df.loc[index, \"task\"] = r.json()[\"url\"]\n",
    "        except:\n",
    "            print(\"Error, file post for {} failed, {}\".format(index, r.json()))\n",
    "            print('Moving on with next timestep...')\n",
    "            continue\n",
    "        \n",
    "        waittime = 0\n",
    "        #Set first status manually\n",
    "        upload_df.loc[index,\"status\"]=\"UNKNOWN\"\n",
    "        while (upload_df.loc[index,\"status\"] not in [\"SUCCESS\", \"FAILURE\"]) and (waittime <= MAXWAITTIME):\n",
    "            waittime += 10\n",
    "            time.sleep(10)\n",
    "            try:\n",
    "                r = requests.get(upload_df.loc[index, \"task\"])\n",
    "                upload_df.loc[index, \"status\"] = r.json()[\"status\"]\n",
    "                upload_df.loc[index, \"waittime\"] = waittime\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        print(\"Moving on with status {}\".format(upload_df.loc[index, \"status\"]))\n",
    "    \n",
    "    return upload_df\n",
    "\n",
    "def str_to_int(string):\n",
    "    try:\n",
    "        integer = int(string)\n",
    "    except:\n",
    "        integer = 0\n",
    "    \n",
    "    return integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario's voor workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarionummers = [14034, 14033, 21007, 21008, 21009, 21010, 21011, 21012, 21014, 21013, 19632, 19633, 5196, 19391, 20677, 20680, 14014, 14039, 4908]\n",
    "all_scenario_zip = r'Y:\\T_Berends\\w0232_liwo_ldo\\ExportLIWO_24112021_1526.zip'\n",
    "output_path = r'C:\\Users\\Emile.deBadts\\Documents\\Projecten\\liwo_data_posten\\scenarios_workshop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(all_scenario_zip)\n",
    "\n",
    "for file in archive.namelist():\n",
    "    file_dir = file.split('/')[0]\n",
    "    if str_to_int(file_dir) in scenarionummers:\n",
    "        archive.extract(file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIWO scenario upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor elk Dordrecht, zoek bijbehorende metadata file als die beschikbaar is. \n",
    "Vervolgens drie variabelen uploaden voor ieder scenario: max. wd (ASCI), max c (ASCI) en temporele waterdiepte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_scenario_folders = os.listdir('./metadata')\n",
    "export_scenario_folders = os.listdir('./eiland_van_dordrecht')\n",
    "additional_metadata = pd.read_csv('scenarios.csv', sep='|')\n",
    "\n",
    "upload_scenarios = [scenario for scenario in export_scenario_folders if scenario in metadata_scenario_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenarios voor workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_scenarios = os.listdir(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .asci files to tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_d_file = 'dm1maxd0.asc'\n",
    "max_c_file = 'dm1maxc0.asc'\n",
    "fls_inc_file = 'fls_h.inc'\n",
    "export_files = [max_d_file, max_c_file, fls_inc_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14014\n",
      "   Create max waterdepth tiff\n",
      "14033\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "14034\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "14039\n",
      "   Create max waterdepth tiff\n",
      "19391\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "19632\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "19633\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "20677\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "20680\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21007\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21008\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21009\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21010\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21011\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21012\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21013\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "21014\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "4908\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n",
      "5196\n",
      "   Create max waterdepth tiff\n",
      "   Create max velocity tiff\n",
      "   Scenario complete!\n"
     ]
    }
   ],
   "source": [
    "complete_scenarios = []\n",
    "\n",
    "for scenario in upload_scenarios:\n",
    "    print(scenario)\n",
    "    scenario_dir = os.path.join(output_path, scenario)\n",
    "    scenario_files = os.listdir(scenario_dir)\n",
    "    \n",
    "    for folder in scenario_files:\n",
    "        if folder.endswith('.zip'):\n",
    "            zip_path = os.path.join(scenario_dir, folder)\n",
    "            archive = zipfile.ZipFile(zip_path)\n",
    "            for file in archive.namelist():\n",
    "                if file in export_files:\n",
    "                    archive.extract(file, scenario_dir)\n",
    "  \n",
    "    scenario_files = os.listdir(scenario_dir)\n",
    "\n",
    "    if max_d_file in scenario_files:\n",
    "        max_waterdepth = True\n",
    "        print('   Create max waterdepth tiff')\n",
    "        input_asci_fn = os.path.join(os.path.abspath(scenario_dir), 'dm1maxd0.asc')\n",
    "        output_fn = os.path.join(scenario_dir, 'max_waterdepth.tif')\n",
    "        asc_to_tif(asc_fn = input_asci_fn, tif_fn = output_fn)\n",
    "    else:\n",
    "        max_waterdepth = False\n",
    "\n",
    "    if max_c_file in scenario_files:\n",
    "        max_velocity = True\n",
    "        print('   Create max velocity tiff')\n",
    "        input_asci_fn = os.path.join(os.path.abspath(scenario_dir), 'dm1maxc0.asc')\n",
    "        output_fn = os.path.join(scenario_dir, 'max_velocity.tif')\n",
    "        asc_to_tif(asc_fn = input_asci_fn, tif_fn = output_fn)\n",
    "    else:\n",
    "        max_velocity = False\n",
    "        \n",
    "    # check fls files\n",
    "#     temporal_wd_files = os.listdir(os.path.join(scenario_dir, 'fls'))\n",
    "    if fls_inc_file in scenario_files:\n",
    "        temporal_wd = True\n",
    "    else:\n",
    "        temporal_wd = False\n",
    "\n",
    "    if all([temporal_wd, max_waterdepth, max_velocity]):\n",
    "        complete_scenarios += [scenario]\n",
    "        print('   Scenario complete!')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new raster sources and post data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "waterdepth_style = \"Blues:0:2\"\n",
    "waterdepth_observation_type = 521\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario maximum waterdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_max_waterdepth = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with scenario 14034\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 14033\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21007\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21008\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21009\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21010\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21011\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21012\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21014\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 21013\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 19632\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 19633\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 5196\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 19391\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 20677\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 20680\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 14014\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 14039\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n",
      "Starting with scenario 4908\n",
      "Using organisation: 2e8ccd61-a0c3-473d-9bda-8b06db2bec35\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data for max waterdepth...\n"
     ]
    }
   ],
   "source": [
    "for scenario in scenarionummers:\n",
    "    \n",
    "    scenario = str(scenario)\n",
    "    scenario_dir = os.path.join(output_path, scenario)\n",
    "    waterdepth_file = os.path.join(scenario_dir, 'max_waterdepth.tif')\n",
    "\n",
    "    if scenario not in posted_max_waterdepth and os.path.exists(waterdepth_file):\n",
    "    \n",
    "        print('Starting with scenario {}'.format(scenario))\n",
    "\n",
    "        metadata = os.path.join(scenario_dir, 'metadata.csv') \n",
    "        metadata_df = pd.read_csv(metadata, sep= '|')\n",
    "\n",
    "        # Data voor in name attribuut\n",
    "        naam = metadata_df.loc['Scenarionaam', 'Scenario']\n",
    "        overschrijding = metadata_df.loc['Overschrijdingsfrequentie', 'Scenario']\n",
    "        naam_buitenwater = metadata_df.loc['Naam buitenwater', 'Scenario']\n",
    "        buitenwatertype = metadata_df.loc['Buitenwatertype', 'Scenario']\n",
    "        gebiedsnaam = metadata_df.loc['Gebiedsnaam', 'Scenario']\n",
    "        naam_doorbraaklocatie = metadata_df.loc['Naam doorbraaklocatie', 'Scenario']\n",
    "        \n",
    "        name_string_format = f'{naam}_Variabele:MaximaleWaterdiepte_Overschrijdingsfrequentie:{overschrijding}_Naambuitenwater:{naam_buitenwater}_Buitenwatertype:{buitenwatertype}_Gebiedsnaam:{gebiedsnaam}_ID:{scenario}'\n",
    "        metadata_json = json.dumps({row.name: row.Scenario for i,row in metadata_df.iterrows()})\n",
    "\n",
    "        \n",
    "        # Get the organisation UUID \n",
    "        #organisatie = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "        #organisation_uuid = search_organisation_by_name(organisatie)\n",
    "        #if organisation_uuid is None:\n",
    "        #    print('No organisation found for the current scenario')\n",
    "        #    break\n",
    "        organisation_uuid = '2e8ccd61-a0c3-473d-9bda-8b06db2bec35'\n",
    "        \n",
    "        print('Using organisation: {}'.format(organisation_uuid))\n",
    "            \n",
    "        # Get or post the layer collection\n",
    "        #project = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "        #projectnaam = metadata_df.loc['Projectnaam', 'Scenario']\n",
    "        #layer_collection_slug = r = create_or_return_layer_collection(slug=projectnaam, organisation_uuid=organisation_uuid)\n",
    "        #print('Using layer collection: {}'.format(layer_collection_slug))\n",
    "        layer_collection_slug = 'LDO_DEMO'\n",
    "        \n",
    "        # Max waterdepth \n",
    "\n",
    "        # Post the source \n",
    "        print('   Posting raster source for max waterdepth...')\n",
    "        r_source_post = post_raster_source(name = name_string_format, \n",
    "                                           description = metadata_json, \n",
    "                                           organisation_uuid = organisation_uuid, \n",
    "                                           temporal = False)\n",
    "\n",
    "        r_source_post.raise_for_status()\n",
    "\n",
    "        # Post raster layer\n",
    "        print('   Posting raster layer for max waterdepth...')\n",
    "        r_layer_post = post_raster_layer(name = name_string_format, \n",
    "                                         description = metadata_json, \n",
    "                                         organisation_uuid = organisation_uuid, \n",
    "                                         observation_type = waterdepth_observation_type, \n",
    "                                         source_uuid = r_source_post.json()['uuid'], \n",
    "                                         style = waterdepth_style,\n",
    "                                         layer_collection_slug=layer_collection_slug)\n",
    "\n",
    "        r_layer_post.raise_for_status()\n",
    "\n",
    "        # Post waterdepth tif\n",
    "        print('   Posting data for max waterdepth...')\n",
    "        r_data_post = post_data_to_source(source_uuid = r_source_post.json()['uuid'], file = waterdepth_file)\n",
    "\n",
    "        r_data_post.raise_for_status()\n",
    "\n",
    "        posted_max_waterdepth += [scenario]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario maximum velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_flow_velocity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_velocity_style = \"Oranges:0:2\"\n",
    "max_velocity_observation_type = 969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with scenario 14033\n",
      "Using organisation: Provincie Zuid-Holland\n",
      "Using layer collection: Gebiedsontwikeling Westergouwe\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 14034\n",
      "Using organisation: Provincie Zuid-Holland\n",
      "Using layer collection: Gebiedsontwikeling Nieuwerkerk Noord\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 19391\n",
      "Using organisation: Provincie Noord-Brabant\n",
      "Using layer collection: LIWO Noord-Brabant\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 19632\n",
      "Using organisation: Provincie Zuid-Holland\n",
      "Using layer collection: Nut en noodzaak compartimenteringskeringen\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 19633\n",
      "Using organisation: Provincie Zuid-Holland\n",
      "Using layer collection: Nut en noodzaak compartimenteringskeringen\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 20677\n",
      "Using organisation: Provincie Utrecht\n",
      "Using layer collection: VNK2 Utrecht\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 20680\n",
      "Using organisation: Provincie Utrecht\n",
      "Using layer collection: VNK2 Utrecht\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21007\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21008\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21009\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21010\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21011\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21012\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21013\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 21014\n",
      "Using organisation: HHRS van Schieland en de Krimpenerwaard\n",
      "Using layer collection: KIJK\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 4908\n",
      "Using organisation: Provincie Noord-Brabant\n",
      "Using layer collection: VNK2 Noord-Brabant\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n",
      "Starting with scenario 5196\n",
      "Using organisation: Provincie Noord-Brabant\n",
      "Using layer collection: VNK2 Noord-Brabant\n",
      "   Posting raster source for max flow velocity...\n",
      "   Posting raster layer for max flow velocity...\n",
      "   Posting data for max flow velocity...\n"
     ]
    }
   ],
   "source": [
    "for scenario in complete_scenarios:\n",
    "    \n",
    "    scenario_dir = os.path.join(output_path, scenario)\n",
    "    velocity_file = os.path.join(scenario_dir, 'max_velocity.tif')\n",
    "    \n",
    "    if scenario not in posted_flow_velocity and os.path.exists(velocity_file):\n",
    "    \n",
    "        print('Starting with scenario {}'.format(scenario))\n",
    "\n",
    "        metadata = os.path.join(scenario_dir, 'metadata.csv') \n",
    "        metadata_df = pd.read_csv(metadata, sep= '|')\n",
    "\n",
    "        naam = metadata_df.loc['Scenarionaam', 'Scenario']\n",
    "        variabele = 'MaximaleStroomsnelheid'\n",
    "        overschrijding = metadata_df.loc['Overschrijdingsfrequentie', 'Scenario']\n",
    "        naam_buitenwater = metadata_df.loc['Naam buitenwater', 'Scenario']\n",
    "        buitenwatertype = metadata_df.loc['Buitenwatertype', 'Scenario']\n",
    "        gebiedsnaam = metadata_df.loc['Gebiedsnaam', 'Scenario']\n",
    "        naam_doorbraaklocatie = metadata_df.loc['Naam doorbraaklocatie', 'Scenario']\n",
    "\n",
    "        name_string_format = f'{naam}_Variabele:{variabele}_Overschrijdingsfrequentie:{overschrijding}_Naambuitenwater:{naam_buitenwater}_Buitenwatertype:{buitenwatertype}_Gebiedsnaam:{gebiedsnaam}_ID:{scenario}'\n",
    "        metadata_json = json.dumps({row.name: row.Scenario for i,row in metadata_df.iterrows()})\n",
    "\n",
    "        # Get the organisation UUID \n",
    "        organisatie = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "        organisation_uuid = search_organisation_by_name(organisatie)\n",
    "        if organisation_uuid is None:\n",
    "            print('No organisation found for the current scenario')\n",
    "            break\n",
    "        \n",
    "        print('Using organisation: {}'.format(organisatie))\n",
    "            \n",
    "        # Get or post the layer collection\n",
    "        project = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "        projectnaam = metadata_df.loc['Projectnaam', 'Scenario']\n",
    "        layer_collection_slug = r = create_or_return_layer_collection(slug=projectnaam, organisation_uuid=organisation_uuid)\n",
    "        print('Using layer collection: {}'.format(layer_collection_slug))\n",
    "\n",
    "        \n",
    "        # Max waterdepth \n",
    "\n",
    "        # Post the source \n",
    "        print('   Posting raster source for max flow velocity...')\n",
    "        r_source_post = post_raster_source(name = name_string_format, \n",
    "                                           description = metadata_json, \n",
    "                                           organisation_uuid = organisation_uuid, \n",
    "                                           temporal = False)\n",
    "\n",
    "        r_source_post.raise_for_status()\n",
    "\n",
    "        # Post raster layer\n",
    "        print('   Posting raster layer for max flow velocity...')\n",
    "        r_layer_post = post_raster_layer(name = name_string_format, \n",
    "                                         description = metadata_json, \n",
    "                                         organisation_uuid = organisation_uuid, \n",
    "                                         observation_type = max_velocity_observation_type, \n",
    "                                         source_uuid = r_source_post.json()['uuid'], \n",
    "                                         style = max_velocity_style,\n",
    "                                         layer_collection_slug=layer_collection_slug)\n",
    "\n",
    "        r_layer_post.raise_for_status()\n",
    "\n",
    "        # Post waterdepth tif\n",
    "        print('   Posting data for max flow velocity...')\n",
    "        r_data_post = post_data_to_source(source_uuid = r_source_post.json()['uuid'], file = velocity_file)\n",
    "\n",
    "        r_data_post.raise_for_status()\n",
    "\n",
    "        posted_flow_velocity += [scenario]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario temporal waterdepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_temporal_waterdepth_data = []\n",
    "temporal_waterdepth_source_uuids = {}\n",
    "temporal_waterdepth_layer_uuids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with scenario 14033\n",
      "Using organisation: Provincie Zuid-Holland\n",
      "Using layer collection: Gebiedsontwikeling Westergouwe\n",
      "   Posting raster source for max waterdepth...\n",
      "   Posting raster layer for max waterdepth...\n",
      "   Posting data to the SOURCE :O\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data '33239.0' does not match format '%d/%m/%Y %H:%M'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-018397891ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0meinde_simulatie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Einde simulatie'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Scenario'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mstart_simulatie_datetime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_simulatie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%d/%m/%Y %H:%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0meind_simulatie_datetime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meinde_simulatie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%d/%m/%Y %H:%M\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[0;32m    350\u001b[0m                          (data_string, format))\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data '33239.0' does not match format '%d/%m/%Y %H:%M'"
     ]
    }
   ],
   "source": [
    "for scenario in complete_scenarios:\n",
    "\n",
    "    print('Starting with scenario {}'.format(scenario))\n",
    "\n",
    "    scenario_dir = os.path.join(output_path, scenario)\n",
    "    metadata = os.path.join(scenario_dir, 'metadata.csv') \n",
    "    metadata_df = pd.read_csv(metadata, sep= '|')\n",
    "\n",
    "    naam = metadata_df.loc['Scenarionaam', 'Scenario']\n",
    "    variabele = 'TemporeleWaterdiepte'\n",
    "    overschrijding = metadata_df.loc['Overschrijdingsfrequentie', 'Scenario']\n",
    "    naam_buitenwater = metadata_df.loc['Naam buitenwater', 'Scenario']\n",
    "    buitenwatertype = metadata_df.loc['Buitenwatertype', 'Scenario']\n",
    "    gebiedsnaam = metadata_df.loc['Gebiedsnaam', 'Scenario']\n",
    "    naam_doorbraaklocatie = metadata_df.loc['Naam doorbraaklocatie', 'Scenario']\n",
    "\n",
    "    name_string_format = f'{naam}_Variabele:{variabele}_Overschrijdingsfrequentie:{overschrijding}_Naambuitenwater:{naam_buitenwater}_Buitenwatertype:{buitenwatertype}_Gebiedsnaam:{gebiedsnaam}_ID:{scenario}'\n",
    "    metadata_json = json.dumps({row.name: row.Scenario for i,row in metadata_df.iterrows()})\n",
    "\n",
    "    # Get the organisation UUID \n",
    "    organisatie = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "    organisation_uuid = search_organisation_by_name(organisatie)\n",
    "    if organisation_uuid is None:\n",
    "        print('No organisation found for the current scenario')\n",
    "        break\n",
    "\n",
    "    print('Using organisation: {}'.format(organisatie))\n",
    "\n",
    "    # Get or post the layer collection\n",
    "    project = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "    projectnaam = metadata_df.loc['Projectnaam', 'Scenario']\n",
    "    layer_collection_slug = r = create_or_return_layer_collection(slug=projectnaam, organisation_uuid=organisation_uuid)\n",
    "    print('Using layer collection: {}'.format(layer_collection_slug))\n",
    "\n",
    "    # Post the source \n",
    "\n",
    "    if scenario not in temporal_waterdepth_source_uuids.keys():\n",
    "\n",
    "        print('   Posting raster source for max waterdepth...')\n",
    "        r_source_post = post_raster_source(name = name_string_format, \n",
    "                                           description = metadata_json, \n",
    "                                           organisation_uuid = organisation_uuid, \n",
    "                                           temporal = True,\n",
    "                                           interval = '01:00:00')\n",
    "\n",
    "        r_source_post.raise_for_status()\n",
    "        temporal_waterdepth_source_uuids[scenario] = r_source_post.json()['uuid']\n",
    "\n",
    "\n",
    "    if scenario not in temporal_waterdepth_layer_uuids.keys():\n",
    "        # Post raster layer\n",
    "        print('   Posting raster layer for max waterdepth...')\n",
    "        r_layer_post = post_raster_layer(name = name_string_format, \n",
    "                                         description = metadata_json, \n",
    "                                         organisation_uuid = organisation_uuid, \n",
    "                                         observation_type = waterdepth_observation_type, \n",
    "                                         source_uuid = temporal_waterdepth_source_uuids[scenario], \n",
    "                                         style = waterdepth_style,\n",
    "                                         layer_collection_slug=layer_collection_slug)\n",
    "\n",
    "        r_layer_post.raise_for_status()\n",
    "        temporal_waterdepth_layer_uuids[scenario] = r_layer_post.json()['uuid']\n",
    "\n",
    "    # Post all waterdepths\n",
    "    \n",
    "    if scenario not in posted_temporal_waterdepth_data:\n",
    "        print('   Posting data to the SOURCE :O')\n",
    "    \n",
    "        # Construct file dataframe with datetime\n",
    "        start_simulatie = metadata_df.loc['Start simulatie', 'Scenario']\n",
    "        einde_simulatie = metadata_df.loc['Einde simulatie', 'Scenario']\n",
    "        \n",
    "        try:\n",
    "            start_simulatie_datetime = datetime.datetime.strptime(start_simulatie, \"%d/%m/%Y %H:%M\")\n",
    "            eind_simulatie_datetime = datetime.datetime.strptime(einde_simulatie, \"%d/%m/%Y %H:%M\")\n",
    "        except:\n",
    "            print('No compatible datetime string')\n",
    "        temporal_files = os.listdir(os.path.join(scenario_dir, 'fls'))\n",
    "        temporal_wd_files = [os.path.join(scenario_dir, 'fls', file) for file in temporal_files if file.startswith('dataset') and file.endswith('.tiff')]\n",
    "\n",
    "        for file in temporal_wd_files:\n",
    "            set_crs_for_tif(file)\n",
    "\n",
    "        datetime_range = [start_simulatie_datetime + datetime.timedelta(hours=x) for x in range(len(temporal_wd_files))]\n",
    "\n",
    "        file_df = pd.DataFrame({'datetime':datetime_range, 'file':temporal_wd_files})\n",
    "        file_df.set_index(file_df.datetime, inplace=True)\n",
    "\n",
    "        url = RASTER_SOURCES_DATA_ENDPOINT.format(uuid=temporal_waterdepth_source_uuids[scenario])\n",
    "\n",
    "        upload_df = upload_rasterperiod(url=url, df=file_df)        \n",
    "        posted_temporal_waterdepth_data += [scenario]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove raster layer and source by ID and variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-76c96772379f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mscenario\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscenario\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdelete_raster_with_scenario_id_and_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscenario_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscenario\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-41044f66683a>\u001b[0m in \u001b[0;36mdelete_raster_with_scenario_id_and_variable\u001b[1;34m(scenario_id, variable, headers)\u001b[0m\n\u001b[0;32m     79\u001b[0m               'name__icontains': variable}\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://demo.lizard.net/api/v4/rasters'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 resp = self.send(\n\u001b[0m\u001b[0;32m    238\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1240\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\work\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "variables = ['MaximaleWaterdiepte', 'MaximaleStroomsnelheid']\n",
    "for scenario in scenarionummers:\n",
    "    for variable in variables:\n",
    "        scenario = str(scenario)\n",
    "        delete_raster_with_scenario_id_and_variable(scenario_id=scenario, variable=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing temporal waterdepth posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = '19749'\n",
    "uuid_temporal_wd = get_rastersource_uuid_scenario(scenario, 'TemporeleWaterdiepte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using organisation: Provincie Zuid-Holland\n",
      "Posting data...\n",
      "Posting for 1991-01-06 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-06 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-07 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-08 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-09 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-10 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 11:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-11 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-12 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-13 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-14 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-15 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-16 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-17 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-18 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-19 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-20 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 13:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-21 23:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 00:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 01:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 02:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 03:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 04:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 05:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 06:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 07:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 08:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 09:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 10:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 11:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 12:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 13:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 14:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 15:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 16:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 17:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 18:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 19:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 20:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 21:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 22:00:00\n",
      "Moving on with status SUCCESS\n",
      "Posting for 1991-01-22 23:00:00\n",
      "Moving on with status SUCCESS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'posted_temporal_waterdepth_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-076a9409c2f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mupload_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupload_rasterperiod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m \u001b[0mposted_temporal_waterdepth_data\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscenario\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'posted_temporal_waterdepth_data' is not defined"
     ]
    }
   ],
   "source": [
    "scenario_dir = os.path.join('./eiland_van_dordrecht/', scenario)\n",
    "metadata = os.path.join('./metadata', scenario,'metadata.csv') \n",
    "metadata_df = pd.read_csv(metadata, sep= '|')\n",
    "\n",
    "naam = metadata_df.loc['Scenarionaam', 'Scenario']\n",
    "variabele = 'TemporeleWaterdiepte'\n",
    "overschrijding = metadata_df.loc['Overschrijdingsfrequentie', 'Scenario']\n",
    "naam_buitenwater = metadata_df.loc['Naam buitenwater', 'Scenario']\n",
    "buitenwatertype = metadata_df.loc['Buitenwatertype', 'Scenario']\n",
    "gebiedsnaam = metadata_df.loc['Gebiedsnaam', 'Scenario']\n",
    "naam_doorbraaklocatie = metadata_df.loc['Naam doorbraaklocatie', 'Scenario']\n",
    "\n",
    "name_string_format = f'{naam}_Variabele:{variabele}_Overschrijdingsfrequentie:{overschrijding}_Naambuitenwater:{naam_buitenwater}_Buitenwatertype:{buitenwatertype}_Gebiedsnaam:{gebiedsnaam}_ID:{scenario}'\n",
    "metadata_json = json.dumps({row.name: row.Scenario for i,row in metadata_df.iterrows()})\n",
    "\n",
    "# Get the organisation UUID \n",
    "organisatie = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "organisation_uuid = search_organisation_by_name(organisatie)\n",
    "if organisation_uuid is None:\n",
    "    print('No organisation found for the current scenario')\n",
    "    exit\n",
    "\n",
    "print('Using organisation: {}'.format(organisatie))\n",
    "\n",
    "# Get or post the layer collection\n",
    "# project = metadata_df.loc['Eigenaar overstromingsinformatie', 'Scenario']\n",
    "# projectnaam = metadata_df.loc['Projectnaam', 'Scenario']\n",
    "# layer_collection_slug = r = create_or_return_layer_collection(slug=projectnaam, organisation_uuid=organisation_uuid)\n",
    "# print('Using layer collection: {}'.format(layer_collection_slug))\n",
    "\n",
    "# # Post the source \n",
    "\n",
    "# print('   Posting raster source for max waterdepth...')\n",
    "# r_source_post = post_raster_source(name = name_string_format, \n",
    "#                                    description = metadata_json, \n",
    "#                                    organisation_uuid = organisation_uuid, \n",
    "#                                    temporal = True,\n",
    "#                                    interval = '01:00:00')\n",
    "\n",
    "# r_source_post.raise_for_status()\n",
    "# temporal_waterdepth_source_uuids[scenario] = r_source_post.json()['uuid']\n",
    "\n",
    "\n",
    "# # Post raster layer\n",
    "# print('   Posting raster layer for max waterdepth...')\n",
    "# r_layer_post = post_raster_layer(name = name_string_format, \n",
    "#                                  description = metadata_json, \n",
    "#                                  organisation_uuid = organisation_uuid, \n",
    "#                                  observation_type = waterdepth_observation_type, \n",
    "#                                  source_uuid = temporal_waterdepth_source_uuids[scenario], \n",
    "#                                  style = waterdepth_style,\n",
    "#                                  layer_collection_slug=layer_collection_slug)\n",
    "\n",
    "# r_layer_post.raise_for_status()\n",
    "# temporal_waterdepth_layer_uuids[scenario] = r_layer_post.json()['uuid']\n",
    "\n",
    "\n",
    "# Construct file dataframe with datetime\n",
    "start_simulatie = metadata_df.loc['Start simulatie', 'Scenario']\n",
    "einde_simulatie = metadata_df.loc['Einde simulatie', 'Scenario']\n",
    "\n",
    "start_simulatie_datetime = datetime.datetime.strptime(start_simulatie, \"%d/%m/%Y %H:%M\")\n",
    "eind_simulatie_datetime = datetime.datetime.strptime(einde_simulatie, \"%d/%m/%Y %H:%M\")\n",
    "\n",
    "print('Posting data...')\n",
    "temporal_files = os.listdir(os.path.join(scenario_dir, 'fls'))\n",
    "temporal_wd_files = [os.path.join(scenario_dir, 'fls', file) for file in temporal_files if file.startswith('dataset') and file.endswith('.tiff')]\n",
    "\n",
    "for file in temporal_wd_files:\n",
    "    set_crs_for_tif(file)\n",
    "\n",
    "datetime_range = [start_simulatie_datetime + datetime.timedelta(hours=x) for x in range(len(temporal_wd_files))]\n",
    "\n",
    "file_df = pd.DataFrame({'datetime':datetime_range, 'file':temporal_wd_files})\n",
    "file_df.set_index(file_df.datetime, inplace=True)\n",
    "\n",
    "url = RASTER_SOURCES_DATA_ENDPOINT.format(uuid=uuid_temporal_wd)\n",
    "\n",
    "upload_df = upload_rasterperiod(url=url, df=file_df)        \n",
    "posted_temporal_waterdepth_data += [scenario]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21014\n",
      "... MaximaleWaterdiepte\n",
      "... MaximaleStroomsnelheid\n"
     ]
    }
   ],
   "source": [
    "variables = ['MaximaleWaterdiepte', 'MaximaleStroomsnelheid']\n",
    "\n",
    "rasterlayer_url = 'https://demo.lizard.net/api/v4/rasters/{}'\n",
    "rastersource_url = 'https://demo.lizard.net/api/v4/rastersources/{}'\n",
    "\n",
    "# Patch organisations\n",
    "for scenario in scenario_select:\n",
    "    scenario = str(scenario)\n",
    "    for variable in variables:\n",
    "        \n",
    "        print('...',variable)\n",
    "        \n",
    "        try:\n",
    "            rastersource_uuid = get_rastersource_uuid_scenario(scenario_id=scenario, variable=variable)\n",
    "            rasterlayer_uuid = get_rasterlayer_uuid_scenario(scenario_id=scenario, variable=variable)\n",
    "        except:\n",
    "            print('Not found')\n",
    "            continue\n",
    "        \n",
    "        r_rl = requests.patch(rasterlayer_url.format(rasterlayer_uuid), \n",
    "                              data=json.dumps({'organisation':'2e8ccd61-a0c3-473d-9bda-8b06db2bec35'}),\n",
    "                              headers=HEADERS)\n",
    "        r_rl.raise_for_status()\n",
    "        \n",
    "        r_rs = requests.patch(rastersource_url.format(rastersource_uuid), \n",
    "                              data=json.dumps({'organisation':'2e8ccd61-a0c3-473d-9bda-8b06db2bec35'}),\n",
    "                              headers=HEADERS)\n",
    "        r_rs.raise_for_status()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data naar staging posten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch raster layer with Group geoblock with statistic\n",
    "layer_url = 'https://demo.lizard.net/api/v4/rasters/846fde77-19bc-496a-8337-c77c9e9543e9/'\n",
    "\n",
    "with open('group_with_statistic_graph.json') as f:\n",
    "    group_geoblock = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, scenario in enumerate(scenarionummers):\n",
    "    scenario = str(scenario)\n",
    "    source = get_rastersource_uuid_scenario(scenario_id=scenario, variable='MaximaleWaterdiepte')\n",
    "    rasterblock = ['lizard_nxt.blocks.LizardRasterSource', source]\n",
    "    rasterblock_name = 'waterdepth_' + str(i)\n",
    "    group_geoblock['graph'][rasterblock_name] = rasterblock\n",
    "    group_geoblock['graph']['group'].append(rasterblock_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mask',\n",
       " 'graph': {'group': ['dask_geomodeling.raster.reduction.Max',\n",
       "   'waterdepth_0',\n",
       "   'waterdepth_1',\n",
       "   'waterdepth_2',\n",
       "   'waterdepth_3',\n",
       "   'waterdepth_4',\n",
       "   'waterdepth_5',\n",
       "   'waterdepth_6',\n",
       "   'waterdepth_7',\n",
       "   'waterdepth_8',\n",
       "   'waterdepth_9',\n",
       "   'waterdepth_10',\n",
       "   'waterdepth_11',\n",
       "   'waterdepth_12',\n",
       "   'waterdepth_13',\n",
       "   'waterdepth_14',\n",
       "   'waterdepth_15',\n",
       "   'waterdepth_16',\n",
       "   'waterdepth_17',\n",
       "   'waterdepth_18'],\n",
       "  'mask': ['dask_geomodeling.raster.misc.MaskBelow', 'group', 0.01],\n",
       "  'waterdepth_0': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '024c8fb2-b155-42ee-a7db-0dca0d1e1bfe'],\n",
       "  'waterdepth_1': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '7fc7196f-8bf1-4ddf-8dd9-6a9b3422a86b'],\n",
       "  'waterdepth_2': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'c8c460aa-a487-4375-b84a-0802213fed6e'],\n",
       "  'waterdepth_3': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'e4ebb1dc-da20-4e60-9b98-4e263f54db7e'],\n",
       "  'waterdepth_4': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'bc6358a4-99c1-41a3-8d99-f22ebb735171'],\n",
       "  'waterdepth_5': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '4fba551b-82b7-43d2-a2eb-45cff6553b13'],\n",
       "  'waterdepth_6': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '39333be2-b815-4902-a946-f49f760ff435'],\n",
       "  'waterdepth_7': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '70760f3d-1829-4021-9e73-22f9140afd3c'],\n",
       "  'waterdepth_8': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '8e709c15-d44d-4eed-b387-f6aa3c76cbd3'],\n",
       "  'waterdepth_9': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '7ce02e8e-36c1-4abb-b4cd-c6a50f25ad7c'],\n",
       "  'waterdepth_10': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '730f62e3-06fa-43c5-b757-f107bf8d492f'],\n",
       "  'waterdepth_11': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'd4e2b49c-0015-4589-a22c-ce20ae2072aa'],\n",
       "  'waterdepth_12': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '94875f6a-c420-432f-940f-16fd3b8e87ce'],\n",
       "  'waterdepth_13': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '0b3fb16f-ef0d-4c39-b105-a600d077b7e8'],\n",
       "  'waterdepth_14': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '2f8c4b8a-f9c1-4c9d-94f9-8e605ca4fce6'],\n",
       "  'waterdepth_15': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'ee4ef54f-c63b-468b-9abd-c1e17caaeeb0'],\n",
       "  'waterdepth_16': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   'f80c1825-fae7-4e6e-bbb8-41e2a62f1739'],\n",
       "  'waterdepth_17': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '4d1fc4ed-6f04-4b6c-b0ac-f4f82262e369'],\n",
       "  'waterdepth_18': ['lizard_nxt.blocks.LizardRasterSource',\n",
       "   '8d870a4e-9590-40b1-bd84-b111eb1bb706']}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_geoblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.patch(layer_url, data=json.dumps({'source':group_geoblock}), headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"mask\", \"graph\": {\"group\": [\"dask_geomodeling.raster.reduction.Max\", \"waterdepth_0\", \"waterdepth_1\", \"waterdepth_2\", \"waterdepth_3\"], \"mask\": [\"dask_geomodeling.raster.misc.MaskBelow\", \"group\", 0.01], \"waterdepth_0\": [\"lizard_nxt.blocks.LizardRasterSource\", \"024c8fb2-b155-42ee-a7db-0dca0d1e1bfe\"], \"waterdepth_1\": [\"lizard_nxt.blocks.LizardRasterSource\", \"7fc7196f-8bf1-4ddf-8dd9-6a9b3422a86b\"], \"waterdepth_2\": [\"lizard_nxt.blocks.LizardRasterSource\", \"c8c460aa-a487-4375-b84a-0802213fed6e\"], \"waterdepth_3\": [\"lizard_nxt.blocks.LizardRasterSource\", \"e4ebb1dc-da20-4e60-9b98-4e263f54db7e\"]}}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(group_geoblock)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
